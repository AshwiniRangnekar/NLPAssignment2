{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just like in P2(a), perform POS Tagging on the Brown corpus. (Like before, train your Logistic Regression model on the\n",
    "#tagged corpus, and test on the untagged one). \n",
    "#Use one vs all logistic regression to perform this exercise. \n",
    "#Essentially, given a word, try to classify it with classifiers trained for all pos tags and get most probable one.\n",
    "#Do NOT use any ML libraries like scipy for coding up the logistic regression. NLTK maybe allowed, but only for \n",
    "#getting corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------Vocabulary of words------------------------------------------------\n",
    "def numeric(stri):\n",
    "    for i in range(len(stri)):\n",
    "        if stri[i].isdigit():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def PreprocessData(data):\n",
    "    temp = []\n",
    "    for i in data:\n",
    "        if i[0] != i[1] and not i[0].isdigit() and numeric(i[0]):\n",
    "            if '-' in i[1]:\n",
    "                ll = i[1].split('-')\n",
    "                brownTag = ll[0]\n",
    "            elif '+' in i[1]:\n",
    "                ll = i[1].split('+')\n",
    "                brownTag = ll[0]\n",
    "            else:\n",
    "                brownTag = i[1]\n",
    "            \n",
    "            brownTag = brownTag.replace('*','')\n",
    "            brownTag = brownTag.replace('$','')\n",
    "#             print((i[0],brownTag))\n",
    "            temp.append((i[0],brownTag))\n",
    "        elif i[0] == '.':\n",
    "            temp.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return temp\n",
    "\n",
    "# ----------------------------------- Sentences ----------------------------------------------------------\n",
    "\n",
    "def form_sentences(pdata):\n",
    "    global sentences\n",
    "    sent = []\n",
    "    #sent.append(tuple(start_sentence1))\n",
    "    #sent.append(tuple(start_sentence2))\n",
    "    for i in pdata:\n",
    "        vocab.add(i[0])\n",
    "        if i[0] != i[1]:\n",
    "            sent.append(i)\n",
    "        else:\n",
    "            #sent.append(tuple(end_sentence))\n",
    "            sentences.append(sent)\n",
    "            sent = []\n",
    "            #sent.append(tuple(start_sentence1))\n",
    "            #sent.append(tuple(start_sentence2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formTagDictionary(pdata):\n",
    "    global tags_dict\n",
    "    for p in pdata:\n",
    "        tags_dict[start_sentence1[1]] = tags_dict.get(start_sentence1[1],0) + 1\n",
    "        tags_dict[start_sentence2[1]] = tags_dict.get(start_sentence2[1],0) + 1\n",
    "        tags_dict[end_sentence[1]] = tags_dict.get(end_sentence[1],0) + 1\n",
    "        for i in p:\n",
    "            tags_dict[i[1]] = tags_dict.get(i[1],0) + 1\n",
    "        \n",
    "def formTagWordPairDict(pdata):\n",
    "    global word_tag_pair\n",
    "    for p in pdata:\n",
    "        for i in p:\n",
    "            word_tag_pair[(i[1],i[0])] = word_tag_pair.get((i[1],i[0]),0) + 1\n",
    "        \n",
    "def formTagBigrams():\n",
    "    global training_data\n",
    "    for s in training_data:\n",
    "        l = len(s)-2\n",
    "        for i in range(l):\n",
    "            tag_bigram[(s[i][1],s[i+1][1])] = tag_bigram.get((s[i][1],s[i+1][1]),0) + 1\n",
    "        \n",
    "def formTagTrigrams():\n",
    "    global training_data\n",
    "    for s in training_data:\n",
    "        l = len(s)-3\n",
    "        for i in range(l):\n",
    "            tag_trigram[(s[i][1],s[i+1][1],s[i+2][1])]=tag_trigram.get((s[i][1],s[i+1][1],s[i+2][1]),0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### FEATURE EXTRACTION #########################\n",
    "def get_feature(token, token_index, sent):\n",
    "    global token_feature\n",
    "    token_feature = {\n",
    "                    'token'             : token,\n",
    "                    'is_first'          : token_index == 0,\n",
    "                    'is_last'           : token_index == len(sent)-1,\n",
    "\n",
    "                    'is_capitalized'    : token[0].upper() == token[0],\n",
    "                    'is_all_capitalized': token.upper() == token,\n",
    "                    'is_capitals_inside': token[1:].lower() != token[1:],\n",
    "                    'is_numeric'        : token.isdigit(),\n",
    "\n",
    "                    'prefix-1'          : token[0],\n",
    "                    'prefix-2'          : '' if len(token) < 2  else token[:1],\n",
    "\n",
    "                    'suffix-1'          : token[-1],\n",
    "                    'suffix-2'          : '' if len(token) < 2  else token[-2:],\n",
    "\n",
    "                    'prev-token'        : '' if token_index == 0     else sent[token_index - 1][0],\n",
    "                    '2-prev-token'      : '' if token_index <= 1     else sent[token_index - 2][0],\n",
    "\n",
    "                    'next-token'        : '' if token_index == len(sent) - 1     else sent[token_index + 1][0],\n",
    "                    '2-next-token'      : '' if token_index >= len(sent) - 2     else sent[token_index + 2][0]\n",
    "                    }\n",
    "    return  token_feature\n",
    "\n",
    "def feature_extraction(pdata):\n",
    "    features = []\n",
    "    pos_labels = []\n",
    "    \n",
    "    for sent in pdata:\n",
    "        #print(sent)\n",
    "        for token_index, token_pair in enumerate(sent):\n",
    "            features.append(get_feature(token_pair[0],token_index,sent))\n",
    "            pos_labels.append(token_pair[1])\n",
    "#     for feat in features:\n",
    "#         print(feat,\"\\n\")\n",
    "    return features,pos_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(features, target, weights):\n",
    "    cost = np.array([],dtype=np.float128)\n",
    "    scores = np.array([],dtype=np.float128)\n",
    "    scores = np.dot(features, weights)\n",
    "    cost = np.sum( target*scores - np.log(1 + np.exp(scores)) )   \n",
    "    return cost\n",
    "\n",
    "def sigmoid(scores):\n",
    "    return 1/(1 + np.exp(-scores))\n",
    "\n",
    "def logistic_Regression(wordFeature,targetPOS,num_epochs,learning_rate):\n",
    "    targetPOS = np.array(targetPOS)\n",
    "    num_classes = len(TAG_SET)\n",
    "    num_feature = wordFeature.shape[1]\n",
    "    classifier = np.zeros(shape=(num_classes+1,num_feature),dtype=np.float128)\n",
    "    \n",
    "    for c in range(0,num_classes+1):\n",
    "        print(\"Training for label: \",c)\n",
    "        #targetLabel =  (targetPOS==c).astype(int)\n",
    "        if(targetPOS==c):\n",
    "            targetLabel=1\n",
    "        else:\n",
    "            targetLabel=0\n",
    "        weights = np.zeros(wordFeature.shape[1],dtype=np.float128)\n",
    "        for epoch in range(num_epochs):\n",
    "            scores = np.dot(wordFeature,weights)\n",
    "            predictions = sigmoid(scores)\n",
    "            output_error = targetLabel - predictions\n",
    "            gradient = np.dot(wordFeature.T,output_error)\n",
    "            weights += (learning_rate * gradient)\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"log_likelihood: \",costFunction(wordFeature, targetLabel, weights))\n",
    "        classifier[c,:] = weights\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def evaluate(weights,testdata,actualPOS):\n",
    "    actualPOS = np.array(actualPOS)\n",
    "    scores = np.dot(testdata, weights.T)\n",
    "    scores = np.round(sigmoid(scores))\n",
    "    predictions = scores.argmax(axis=1)\t\n",
    "    print ('Accuracy: {0}'.format((predictions == actualPOS).sum().astype(float) / len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------- MAIN FUNCTION ------------------------------------------------------\n",
    "\n",
    "import string\n",
    "\n",
    "sentences = []\n",
    "vocab = set()\n",
    "\n",
    "start_sentence1 = (\"<sos1>\",\"<ssos1>\")\n",
    "start_sentence2 = (\"<sos2>\",\"<ssos2>\")\n",
    "end_sentence = (\"<eos>\",\"<eeos>\")\n",
    "\n",
    "raw_data = brown.tagged_words()\n",
    "processed_data = PreprocessData(raw_data[:400])\n",
    "form_sentences(processed_data)\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i < 0.8*len(sentences):\n",
    "        training_data.append(sentences[i])\n",
    "    else:\n",
    "        testing_data.append(sentences[i])\n",
    "\n",
    "tag_trigram = {}\n",
    "tag_bigram = {}\n",
    "word_tag_pair = {}\n",
    "word_word_tag_tri = {}\n",
    "tags_dict = {}\n",
    "transition_probs = {}\n",
    "emission_probs = {}\n",
    "\n",
    "formTagDictionary(training_data)\n",
    "# formTagWordPairDict(training_data)\n",
    "# formTagTrigrams()\n",
    "# formTagBigrams()\n",
    "# transition_probability()\n",
    "# emission_probability()\n",
    "TAG_SET = list(tags_dict.keys())\n",
    "\n",
    "# print(tag_trigram)\n",
    "# print(tag_bigram)\n",
    "# print(transition_probs)\n",
    "# print(\"\\n\\n\")\n",
    "# print(emission_probs)\n",
    "\n",
    "\n",
    "# print(tags_dict)\n",
    "print(len(tags_dict))\n",
    "# print(word_tag_pair)\n",
    "print(len(training_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for label:  0\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  1\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  2\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  3\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  4\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  5\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  6\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  7\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  8\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  9\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  10\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  11\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  12\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  13\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  14\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  15\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  16\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  17\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  18\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  19\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  20\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  21\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  22\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  23\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  24\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  25\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwini/.local/lib/python3.5/site-packages/ipykernel_launcher.py:20: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  27\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  28\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  29\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  30\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  31\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  32\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  33\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  34\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  35\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  36\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  37\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  38\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  39\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  40\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  41\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "Training for label:  42\n",
      "log_likelihood:  -4.7866974607762764345\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "features,pos_labels = feature_extraction(training_data)\n",
    "v = DictVectorizer(sparse=False)\n",
    "train_x = v.fit_transform(features)\n",
    "weights = logistic_Regression(train_x,pos_labels,2,0.1)\n",
    "print(type(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DictVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3627e8c0e8b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatures1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_labels1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_labels1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tosequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mXa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DictVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "features1,pos_labels1 = feature_extraction(testing_data)\n",
    "v = DictVectorizer(sparse=False)\n",
    "test_x = v.transform(features1)\n",
    "\n",
    "evaluate(weights,test_x,pos_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#one way is using class (optional), advantage being you can create multiple instances of class and train for each pos tag\n",
    "\n",
    "class logisitic_regression:\n",
    "    def __init__():\n",
    "        pass\n",
    "    def train(data,pos_tag):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another crude way make train function for each class\n",
    "def train_for_class_A(data):\n",
    "    #train classifier for each pos tag in one vs all; in this particular case one will be class A and \n",
    "    #other class is rest\n",
    "    pass\n",
    "\n",
    "def train_for_class_B(data):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_corpus(corp):\n",
    "    #Read the Brown Corpus\n",
    "    #Take in one sentence at a time\n",
    "    tokenize_text(sentence)\n",
    "    pass\n",
    "\n",
    "#Consider clas as positive class, and the rest as negative, and perform LR for the given token tok\n",
    "def Logistic(tok, clas):\n",
    "    pass\n",
    "\n",
    "def Multi_Logistic():\n",
    "    read_corpus(corpus)\n",
    "    #code up one-many Logistic Regression (LR).\n",
    "    #Feed in the list of tokens to return the list of tags.\n",
    "    #Essentially take one of the classes as positive, and remaining as negative, and perform the standard LR.\n",
    "    #Repeat this for all the classes.\n",
    "    for token in tokens:\n",
    "        for class1 in classes:\n",
    "            Logistic(token, class1)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 : Predict tag sequence and get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_tag_sequence(sentence):\n",
    "    # Given a sentence, Get sequence of tags for it by getting most prefered tag for each word given  \n",
    "    # Feel free to add helper functions more features ( like say trigram w1w2w3 as features for w2)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
